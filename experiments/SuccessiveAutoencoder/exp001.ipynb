{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:33:05.006004094Z",
     "start_time": "2024-04-13T19:33:03.076791730Z"
    }
   },
   "outputs": [],
   "source": [
    "from lib.DataGenerator import DataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "memory_size = 1024\n",
    "batch_size = 128\n",
    "device = 'cuda'\n",
    "history_size = 1000\n",
    "\n",
    "generator = DataGenerator(\n",
    "    '../../datasets/bas_clean.txt',\n",
    "    batch_size=batch_size,\n",
    "    history_size=history_size,\n",
    "    device=device\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:33:05.234329125Z",
     "start_time": "2024-04-13T19:33:04.990537809Z"
    }
   },
   "id": "56bc3e6c300ff465",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1000, 256])\n"
     ]
    }
   ],
   "source": [
    "x, _ = generator.get_batch()\n",
    "print(x.shape) # (batch_size, history_size, 256)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-13T19:33:05.795726296Z",
     "start_time": "2024-04-13T19:33:05.567895124Z"
    }
   },
   "id": "4eab56e730d4c3d9",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self,batch_size,input_size,memory_size,device):\n",
    "        super(Net, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.memory_size = memory_size\n",
    "        self.device = device\n",
    "        hsize = input_size + memory_size\n",
    "        self.swish = torch.nn.SiLU()\n",
    "        self.l1_f = torch.nn.Linear(input_size+memory_size,hsize).to(device)\n",
    "        self.l2_f = torch.nn.Linear(hsize,memory_size).to(device).to(device)\n",
    "        self.g = \n",
    "        self.memory = torch.zeros((batch_size,memory_size),device=device)\n",
    "\n",
    "    def reset(self):\n",
    "        self.memory = torch.zeros((self.batch_size,self.memory_size),device=self.device)\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.memory = self.memory.detach() #detach the memory to avoid backpropagating through the memory in time\n",
    "        x = torch.cat((x,self.memory),1)\n",
    "        self.memory = torch.tanh(self.f(x))\n",
    "        return self.memory\n",
    "\n",
    "    def reconstruct(self, noise=0):\n",
    "        x = self.g(self.memory + noise * torch.randn_like(self.memory))\n",
    "        pred_memory = x[:,:self.memory_size]\n",
    "        pred_memory = torch.tanh(pred_memory)\n",
    "        pred_input = x[:,self.memory_size:]\n",
    "        return pred_memory, pred_input\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbd42aa654f8f864"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0, input size 1280, output size 1406\n",
      "layer 1, input size 1406, output size 1544\n",
      "layer 2, input size 1544, output size 1697\n",
      "layer 3, input size 1697, output size 1864\n",
      "layer 0, input size 1024, output size 1024\n",
      "layer 1, input size 1024, output size 1024\n",
      "layer 2, input size 1024, output size 1024\n",
      "layer 3, input size 1024, output size 1024\n",
      "layer 0, input size 1024, output size 776\n",
      "layer 1, input size 776, output size 588\n",
      "layer 2, input size 588, output size 445\n",
      "layer 3, input size 445, output size 337\n",
      "it: 0, i: 0, loss: 5.541074752807617, loss_memory: 0.00034740008413791656, loss_input: 5.540727138519287\n",
      "it: 0, i: 1, loss: 5.530224323272705, loss_memory: 0.00016194673662539572, loss_input: 5.530062198638916\n",
      "it: 0, i: 2, loss: 5.5073394775390625, loss_memory: 0.00015232546138577163, loss_input: 5.507187366485596\n",
      "it: 0, i: 3, loss: 5.467586040496826, loss_memory: 0.0003743020643014461, loss_input: 5.467211723327637\n",
      "it: 0, i: 4, loss: 4.6797051429748535, loss_memory: 0.0016364610055461526, loss_input: 4.6780686378479\n",
      "it: 0, i: 5, loss: 45.648353576660156, loss_memory: 0.06473967432975769, loss_input: 45.583614349365234\n",
      "it: 0, i: 6, loss: 15.565982818603516, loss_memory: 0.7755038738250732, loss_input: 14.790478706359863\n",
      "it: 0, i: 7, loss: 5.809318542480469, loss_memory: 0.7641856670379639, loss_input: 5.045133113861084\n",
      "it: 0, i: 8, loss: 5.580957889556885, loss_memory: 0.6321640610694885, loss_input: 4.948793888092041\n",
      "it: 0, i: 9, loss: 5.348446369171143, loss_memory: 0.02708202786743641, loss_input: 5.321364402770996\n",
      "it: 0, i: 10, loss: 5.578614234924316, loss_memory: 0.14195182919502258, loss_input: 5.436662197113037\n",
      "it: 0, i: 11, loss: 5.578151702880859, loss_memory: 0.11444796621799469, loss_input: 5.463703632354736\n",
      "it: 0, i: 12, loss: 5.544228553771973, loss_memory: 0.09186791628599167, loss_input: 5.4523606300354\n",
      "it: 0, i: 13, loss: 5.485368251800537, loss_memory: 0.06260481476783752, loss_input: 5.422763347625732\n",
      "it: 0, i: 14, loss: 5.430058002471924, loss_memory: 0.04093698784708977, loss_input: 5.389121055603027\n",
      "it: 0, i: 15, loss: 5.383717060089111, loss_memory: 0.03089876100420952, loss_input: 5.352818489074707\n",
      "it: 0, i: 16, loss: 5.320952892303467, loss_memory: 0.033728621900081635, loss_input: 5.287224292755127\n",
      "it: 0, i: 17, loss: 5.237946510314941, loss_memory: 0.013085507787764072, loss_input: 5.224861145019531\n",
      "it: 0, i: 18, loss: 5.589653015136719, loss_memory: 0.5991779565811157, loss_input: 4.990475177764893\n",
      "it: 0, i: 19, loss: 4.897773742675781, loss_memory: 0.22679726779460907, loss_input: 4.670976638793945\n",
      "it: 0, i: 20, loss: 4.298648357391357, loss_memory: 0.02715371921658516, loss_input: 4.2714948654174805\n",
      "it: 0, i: 21, loss: 4.904489994049072, loss_memory: 0.034049853682518005, loss_input: 4.8704400062561035\n",
      "it: 0, i: 22, loss: 4.434193134307861, loss_memory: 0.07220029830932617, loss_input: 4.361992835998535\n",
      "it: 0, i: 23, loss: 4.020960330963135, loss_memory: 0.02742033451795578, loss_input: 3.993539810180664\n",
      "it: 0, i: 24, loss: 4.022226333618164, loss_memory: 0.02716798521578312, loss_input: 3.995058536529541\n",
      "it: 0, i: 25, loss: 4.04741907119751, loss_memory: 0.029969800263643265, loss_input: 4.017449378967285\n",
      "it: 0, i: 26, loss: 4.027496337890625, loss_memory: 0.028439469635486603, loss_input: 3.999056816101074\n",
      "it: 0, i: 27, loss: 3.8949337005615234, loss_memory: 0.025860434398055077, loss_input: 3.8690731525421143\n",
      "it: 0, i: 28, loss: 3.8270440101623535, loss_memory: 0.02490435354411602, loss_input: 3.8021397590637207\n",
      "it: 0, i: 29, loss: 3.959350347518921, loss_memory: 0.02232210524380207, loss_input: 3.937028169631958\n",
      "it: 0, i: 30, loss: 3.884014129638672, loss_memory: 0.017748400568962097, loss_input: 3.8662657737731934\n",
      "it: 0, i: 31, loss: 3.948981523513794, loss_memory: 0.013594582676887512, loss_input: 3.935386896133423\n",
      "it: 0, i: 32, loss: 3.8711295127868652, loss_memory: 0.010887153446674347, loss_input: 3.8602423667907715\n",
      "it: 0, i: 33, loss: 3.838611364364624, loss_memory: 0.009522386826574802, loss_input: 3.8290889263153076\n",
      "it: 0, i: 34, loss: 3.806074857711792, loss_memory: 0.008956248871982098, loss_input: 3.797118663787842\n",
      "it: 0, i: 35, loss: 3.8198416233062744, loss_memory: 0.008650240488350391, loss_input: 3.8111913204193115\n",
      "it: 0, i: 36, loss: 3.821049690246582, loss_memory: 0.007956191897392273, loss_input: 3.813093423843384\n",
      "it: 0, i: 37, loss: 3.7554736137390137, loss_memory: 0.006572293117642403, loss_input: 3.7489013671875\n",
      "it: 0, i: 38, loss: 3.747886896133423, loss_memory: 0.005096272565424442, loss_input: 3.742790699005127\n",
      "it: 0, i: 39, loss: 3.8716745376586914, loss_memory: 0.0039696721360087395, loss_input: 3.8677048683166504\n",
      "it: 0, i: 40, loss: 3.853264093399048, loss_memory: 0.003165441332384944, loss_input: 3.8500986099243164\n",
      "it: 0, i: 41, loss: 3.6228177547454834, loss_memory: 0.0026538162492215633, loss_input: 3.620163917541504\n",
      "it: 0, i: 42, loss: 3.786256790161133, loss_memory: 0.002456307876855135, loss_input: 3.7838003635406494\n",
      "it: 0, i: 43, loss: 3.8480288982391357, loss_memory: 0.0024951607920229435, loss_input: 3.845533847808838\n",
      "it: 0, i: 44, loss: 3.8363935947418213, loss_memory: 0.0025811020750552416, loss_input: 3.8338124752044678\n",
      "it: 0, i: 45, loss: 3.6869101524353027, loss_memory: 0.002535186242312193, loss_input: 3.684375047683716\n",
      "it: 0, i: 46, loss: 3.704923152923584, loss_memory: 0.00229960517026484, loss_input: 3.7026236057281494\n",
      "it: 0, i: 47, loss: 3.725417375564575, loss_memory: 0.0019605886191129684, loss_input: 3.723456859588623\n",
      "it: 0, i: 48, loss: 3.74410080909729, loss_memory: 0.0016611337196081877, loss_input: 3.7424397468566895\n",
      "it: 0, i: 49, loss: 3.7312533855438232, loss_memory: 0.0014763305662199855, loss_input: 3.7297770977020264\n",
      "it: 0, i: 50, loss: 3.769521474838257, loss_memory: 0.0013603835832327604, loss_input: 3.7681610584259033\n",
      "it: 0, i: 51, loss: 3.7846333980560303, loss_memory: 0.0012348563177511096, loss_input: 3.7833986282348633\n",
      "it: 0, i: 52, loss: 3.5781021118164062, loss_memory: 0.0011009968584403396, loss_input: 3.5770010948181152\n",
      "it: 0, i: 53, loss: 3.6664018630981445, loss_memory: 0.0010220810072496533, loss_input: 3.665379762649536\n",
      "it: 0, i: 54, loss: 3.67352294921875, loss_memory: 0.0009966727811843157, loss_input: 3.6725263595581055\n",
      "it: 0, i: 55, loss: 3.6504266262054443, loss_memory: 0.0009361568372696638, loss_input: 3.6494903564453125\n",
      "it: 0, i: 56, loss: 3.755207061767578, loss_memory: 0.0008032695623114705, loss_input: 3.754403829574585\n",
      "it: 0, i: 57, loss: 3.769558906555176, loss_memory: 0.0006679914658889174, loss_input: 3.768890857696533\n",
      "it: 0, i: 58, loss: 3.9038498401641846, loss_memory: 0.0005875424831174314, loss_input: 3.9032623767852783\n",
      "it: 0, i: 59, loss: 3.718770742416382, loss_memory: 0.0005429595476016402, loss_input: 3.7182278633117676\n",
      "it: 0, i: 60, loss: 3.70630145072937, loss_memory: 0.0004983897088095546, loss_input: 3.705803155899048\n",
      "it: 0, i: 61, loss: 3.708235740661621, loss_memory: 0.00045805046102032065, loss_input: 3.707777738571167\n",
      "it: 0, i: 62, loss: 3.6440389156341553, loss_memory: 0.0004426345694810152, loss_input: 3.6435961723327637\n",
      "it: 0, i: 63, loss: 3.5839500427246094, loss_memory: 0.0004484733799472451, loss_input: 3.5835015773773193\n",
      "it: 0, i: 64, loss: 3.8382411003112793, loss_memory: 0.0004462781362235546, loss_input: 3.837794780731201\n",
      "it: 0, i: 65, loss: 3.5825531482696533, loss_memory: 0.0004150168679188937, loss_input: 3.5821380615234375\n",
      "it: 0, i: 66, loss: 3.59395170211792, loss_memory: 0.00036358513170853257, loss_input: 3.59358811378479\n",
      "it: 0, i: 67, loss: 3.6081907749176025, loss_memory: 0.0003152472781948745, loss_input: 3.6078755855560303\n",
      "it: 0, i: 68, loss: 3.612614154815674, loss_memory: 0.0002811063313856721, loss_input: 3.612333059310913\n",
      "it: 0, i: 69, loss: 3.8849830627441406, loss_memory: 0.0002544194576330483, loss_input: 3.8847286701202393\n",
      "it: 0, i: 70, loss: 3.592083692550659, loss_memory: 0.00022785793407820165, loss_input: 3.591855764389038\n",
      "it: 0, i: 71, loss: 3.748013734817505, loss_memory: 0.0002070205518975854, loss_input: 3.7478067874908447\n",
      "it: 0, i: 72, loss: 3.6362175941467285, loss_memory: 0.00019909473485313356, loss_input: 3.6360185146331787\n",
      "it: 0, i: 73, loss: 3.517698049545288, loss_memory: 0.00019900861661881208, loss_input: 3.5174989700317383\n",
      "it: 0, i: 74, loss: 3.721015453338623, loss_memory: 0.00019387055363040417, loss_input: 3.7208216190338135\n",
      "it: 0, i: 75, loss: 3.7088546752929688, loss_memory: 0.00017923201085068285, loss_input: 3.7086753845214844\n",
      "it: 0, i: 76, loss: 3.555716037750244, loss_memory: 0.00016271955973934382, loss_input: 3.555553436279297\n",
      "it: 0, i: 77, loss: 3.6075947284698486, loss_memory: 0.00015142410120461136, loss_input: 3.607443332672119\n",
      "it: 0, i: 78, loss: 3.633206367492676, loss_memory: 0.0001439104089513421, loss_input: 3.6330623626708984\n",
      "it: 0, i: 79, loss: 3.5651636123657227, loss_memory: 0.00013544841203838587, loss_input: 3.565028190612793\n",
      "it: 0, i: 80, loss: 3.6323342323303223, loss_memory: 0.00012627866817638278, loss_input: 3.6322078704833984\n",
      "it: 0, i: 81, loss: 3.5085110664367676, loss_memory: 0.00011973179061897099, loss_input: 3.5083913803100586\n",
      "it: 0, i: 82, loss: 3.524503707885742, loss_memory: 0.00011583995365072042, loss_input: 3.524387836456299\n",
      "it: 0, i: 83, loss: 3.7408130168914795, loss_memory: 0.00011075988732045516, loss_input: 3.7407021522521973\n",
      "it: 0, i: 84, loss: 3.53505802154541, loss_memory: 0.00010262206342304125, loss_input: 3.5349555015563965\n",
      "it: 0, i: 85, loss: 3.6361911296844482, loss_memory: 9.405798482475802e-05, loss_input: 3.636096954345703\n",
      "it: 0, i: 86, loss: 3.6430163383483887, loss_memory: 8.872133184922859e-05, loss_input: 3.642927646636963\n",
      "it: 0, i: 87, loss: 3.6004416942596436, loss_memory: 8.648134826216847e-05, loss_input: 3.6003551483154297\n",
      "it: 0, i: 88, loss: 3.5473599433898926, loss_memory: 8.465593418804929e-05, loss_input: 3.5472753047943115\n",
      "it: 0, i: 89, loss: 3.7293031215667725, loss_memory: 8.182378951460123e-05, loss_input: 3.7292213439941406\n",
      "it: 0, i: 90, loss: 3.6481399536132812, loss_memory: 7.9013203503564e-05, loss_input: 3.6480610370635986\n",
      "it: 0, i: 91, loss: 3.813910961151123, loss_memory: 7.6672324212268e-05, loss_input: 3.8138341903686523\n",
      "it: 0, i: 92, loss: 3.49771785736084, loss_memory: 7.358219590969384e-05, loss_input: 3.4976441860198975\n",
      "it: 0, i: 93, loss: 3.3939976692199707, loss_memory: 6.911445962032303e-05, loss_input: 3.3939285278320312\n",
      "it: 0, i: 94, loss: 3.4977500438690186, loss_memory: 6.468532228609547e-05, loss_input: 3.497685432434082\n",
      "it: 0, i: 95, loss: 3.653966188430786, loss_memory: 6.20383070781827e-05, loss_input: 3.6539041996002197\n",
      "it: 0, i: 96, loss: 3.501908779144287, loss_memory: 6.086998837417923e-05, loss_input: 3.501847982406616\n",
      "it: 0, i: 97, loss: 3.529803991317749, loss_memory: 5.982392031000927e-05, loss_input: 3.5297441482543945\n",
      "it: 0, i: 98, loss: 3.560964345932007, loss_memory: 5.828241046401672e-05, loss_input: 3.560906171798706\n",
      "it: 0, i: 99, loss: 3.798290967941284, loss_memory: 5.677801891579293e-05, loss_input: 3.798234224319458\n",
      "it: 0, i: 100, loss: 3.615596294403076, loss_memory: 5.540730489883572e-05, loss_input: 3.6155409812927246\n",
      "it: 0, i: 101, loss: 3.6223843097686768, loss_memory: 5.359404894988984e-05, loss_input: 3.622330665588379\n",
      "it: 0, i: 102, loss: 3.5798871517181396, loss_memory: 5.1360631914576516e-05, loss_input: 3.579835891723633\n",
      "it: 0, i: 103, loss: 3.550968647003174, loss_memory: 4.9434245738666505e-05, loss_input: 3.5509192943573\n",
      "it: 0, i: 104, loss: 3.744130849838257, loss_memory: 4.82871473650448e-05, loss_input: 3.744082450866699\n",
      "it: 0, i: 105, loss: 3.6888952255249023, loss_memory: 4.752563108922914e-05, loss_input: 3.688847780227661\n",
      "it: 0, i: 106, loss: 3.6883442401885986, loss_memory: 4.660835111280903e-05, loss_input: 3.688297748565674\n",
      "it: 0, i: 107, loss: 3.539872646331787, loss_memory: 4.5508444600272924e-05, loss_input: 3.5398271083831787\n",
      "it: 0, i: 108, loss: 3.4280107021331787, loss_memory: 4.446771345101297e-05, loss_input: 3.4279661178588867\n",
      "it: 0, i: 109, loss: 3.6674554347991943, loss_memory: 4.34869944001548e-05, loss_input: 3.667412042617798\n",
      "it: 0, i: 110, loss: 3.5542008876800537, loss_memory: 4.232510764268227e-05, loss_input: 3.5541584491729736\n",
      "it: 0, i: 111, loss: 3.472033977508545, loss_memory: 4.122858081245795e-05, loss_input: 3.4719927310943604\n",
      "it: 0, i: 112, loss: 3.522132635116577, loss_memory: 4.039097984787077e-05, loss_input: 3.522092342376709\n",
      "it: 0, i: 113, loss: 3.4465343952178955, loss_memory: 3.98187548853457e-05, loss_input: 3.4464945793151855\n",
      "it: 0, i: 114, loss: 3.4770209789276123, loss_memory: 3.9231683331308886e-05, loss_input: 3.4769816398620605\n",
      "it: 0, i: 115, loss: 3.545567512512207, loss_memory: 3.854328679153696e-05, loss_input: 3.5455288887023926\n",
      "it: 0, i: 116, loss: 3.5226643085479736, loss_memory: 3.776748781092465e-05, loss_input: 3.5226266384124756\n",
      "it: 0, i: 117, loss: 3.706233263015747, loss_memory: 3.7027311918791384e-05, loss_input: 3.7061963081359863\n",
      "it: 0, i: 118, loss: 3.6086316108703613, loss_memory: 3.63138024113141e-05, loss_input: 3.608595371246338\n",
      "it: 0, i: 119, loss: 3.578005790710449, loss_memory: 3.557362288120203e-05, loss_input: 3.577970266342163\n",
      "it: 0, i: 120, loss: 3.5924019813537598, loss_memory: 3.491901225061156e-05, loss_input: 3.592367172241211\n",
      "it: 0, i: 121, loss: 3.7031407356262207, loss_memory: 3.440377622609958e-05, loss_input: 3.70310640335083\n",
      "it: 0, i: 122, loss: 3.559687376022339, loss_memory: 3.396379906916991e-05, loss_input: 3.5596535205841064\n",
      "it: 0, i: 123, loss: 3.8025598526000977, loss_memory: 3.346677840454504e-05, loss_input: 3.8025264739990234\n",
      "it: 0, i: 124, loss: 3.7040650844573975, loss_memory: 3.296753857284784e-05, loss_input: 3.7040321826934814\n",
      "it: 0, i: 125, loss: 3.8538894653320312, loss_memory: 3.2470528822159395e-05, loss_input: 3.8538570404052734\n",
      "it: 0, i: 126, loss: 3.7116951942443848, loss_memory: 3.19525133818388e-05, loss_input: 3.711663246154785\n",
      "it: 0, i: 127, loss: 3.8244409561157227, loss_memory: 3.146782546536997e-05, loss_input: 3.8244094848632812\n",
      "it: 0, i: 128, loss: 3.6063365936279297, loss_memory: 3.096248838119209e-05, loss_input: 3.6063055992126465\n",
      "it: 0, i: 129, loss: 3.5524396896362305, loss_memory: 3.051222302019596e-05, loss_input: 3.5524091720581055\n",
      "it: 0, i: 130, loss: 3.6913623809814453, loss_memory: 3.0125080229481682e-05, loss_input: 3.6913323402404785\n",
      "it: 0, i: 131, loss: 3.6986052989959717, loss_memory: 2.9723867555730976e-05, loss_input: 3.698575496673584\n",
      "it: 0, i: 132, loss: 3.5282132625579834, loss_memory: 2.9321097827050835e-05, loss_input: 3.528183937072754\n",
      "it: 0, i: 133, loss: 3.618263006210327, loss_memory: 2.8952243155799806e-05, loss_input: 3.618234157562256\n",
      "it: 0, i: 134, loss: 3.5711684226989746, loss_memory: 2.8553091397043318e-05, loss_input: 3.5711398124694824\n",
      "it: 0, i: 135, loss: 3.6739249229431152, loss_memory: 2.8172504244139418e-05, loss_input: 3.6738967895507812\n",
      "it: 0, i: 136, loss: 3.385831594467163, loss_memory: 2.7805770514532924e-05, loss_input: 3.385803699493408\n",
      "it: 0, i: 137, loss: 3.4069716930389404, loss_memory: 2.74629273917526e-05, loss_input: 3.4069442749023438\n",
      "it: 0, i: 138, loss: 3.6378719806671143, loss_memory: 2.7133501134812832e-05, loss_input: 3.6378448009490967\n",
      "it: 0, i: 139, loss: 3.4062132835388184, loss_memory: 2.6799403713084757e-05, loss_input: 3.406186580657959\n",
      "it: 0, i: 140, loss: 3.5438895225524902, loss_memory: 2.646780376380775e-05, loss_input: 3.54386305809021\n",
      "it: 0, i: 141, loss: 3.6298022270202637, loss_memory: 2.6147232347284444e-05, loss_input: 3.6297760009765625\n",
      "it: 0, i: 142, loss: 3.715623140335083, loss_memory: 2.581818262115121e-05, loss_input: 3.71559739112854\n",
      "it: 0, i: 143, loss: 3.566707134246826, loss_memory: 2.5514053049846552e-05, loss_input: 3.5666816234588623\n",
      "it: 0, i: 144, loss: 3.583969831466675, loss_memory: 2.5215718778781593e-05, loss_input: 3.58394455909729\n",
      "it: 0, i: 145, loss: 3.4793050289154053, loss_memory: 2.49177264777245e-05, loss_input: 3.4792799949645996\n",
      "it: 0, i: 146, loss: 3.6958060264587402, loss_memory: 2.4656339519424364e-05, loss_input: 3.6957814693450928\n",
      "it: 0, i: 147, loss: 3.4699316024780273, loss_memory: 2.4372959160245955e-05, loss_input: 3.469907283782959\n",
      "it: 0, i: 148, loss: 3.630133628845215, loss_memory: 2.412147296126932e-05, loss_input: 3.6301095485687256\n",
      "it: 0, i: 149, loss: 3.6773862838745117, loss_memory: 2.3845115720178e-05, loss_input: 3.6773624420166016\n",
      "it: 0, i: 150, loss: 3.6114377975463867, loss_memory: 2.3598298867000267e-05, loss_input: 3.6114141941070557\n",
      "it: 0, i: 151, loss: 3.59527325630188, loss_memory: 2.3322287233895622e-05, loss_input: 3.595249891281128\n",
      "it: 0, i: 152, loss: 3.583635091781616, loss_memory: 2.307762588316109e-05, loss_input: 3.5836119651794434\n",
      "it: 0, i: 153, loss: 3.671703815460205, loss_memory: 2.2841139070806094e-05, loss_input: 3.6716809272766113\n",
      "it: 0, i: 154, loss: 3.6163291931152344, loss_memory: 2.259834946016781e-05, loss_input: 3.6163065433502197\n",
      "it: 0, i: 155, loss: 3.7938296794891357, loss_memory: 2.235714600828942e-05, loss_input: 3.7938072681427\n",
      "it: 0, i: 156, loss: 3.620549201965332, loss_memory: 2.2141892259242013e-05, loss_input: 3.6205270290374756\n",
      "it: 0, i: 157, loss: 3.629818916320801, loss_memory: 2.1893010853091255e-05, loss_input: 3.6297969818115234\n",
      "it: 0, i: 158, loss: 3.647838592529297, loss_memory: 2.1694166207453236e-05, loss_input: 3.6478168964385986\n",
      "it: 0, i: 159, loss: 3.649231195449829, loss_memory: 2.1451320208143443e-05, loss_input: 3.64920973777771\n",
      "it: 0, i: 160, loss: 3.7895939350128174, loss_memory: 2.1219158952590078e-05, loss_input: 3.7895727157592773\n",
      "it: 0, i: 161, loss: 3.390665292739868, loss_memory: 2.1037583792349324e-05, loss_input: 3.3906443119049072\n",
      "it: 0, i: 162, loss: 3.7373740673065186, loss_memory: 2.08269884751644e-05, loss_input: 3.7373533248901367\n",
      "it: 0, i: 163, loss: 3.7426199913024902, loss_memory: 2.0619640054064803e-05, loss_input: 3.7425994873046875\n",
      "it: 0, i: 164, loss: 3.8050527572631836, loss_memory: 2.0426145056262612e-05, loss_input: 3.805032253265381\n",
      "it: 0, i: 165, loss: 3.6736037731170654, loss_memory: 2.0232446331647225e-05, loss_input: 3.673583507537842\n",
      "it: 0, i: 166, loss: 3.6300299167633057, loss_memory: 2.003194094868377e-05, loss_input: 3.630009889602661\n",
      "it: 0, i: 167, loss: 3.6937663555145264, loss_memory: 1.9834053091472015e-05, loss_input: 3.693746566772461\n",
      "it: 0, i: 168, loss: 3.846893787384033, loss_memory: 1.96506007341668e-05, loss_input: 3.846874237060547\n",
      "it: 0, i: 169, loss: 3.6185858249664307, loss_memory: 1.947621422004886e-05, loss_input: 3.6185662746429443\n",
      "it: 0, i: 170, loss: 3.7919118404388428, loss_memory: 1.9307644834043458e-05, loss_input: 3.7918925285339355\n",
      "it: 0, i: 171, loss: 3.5667340755462646, loss_memory: 1.9106293621007353e-05, loss_input: 3.5667150020599365\n",
      "it: 0, i: 172, loss: 3.6954474449157715, loss_memory: 1.8930380974779837e-05, loss_input: 3.6954286098480225\n",
      "it: 0, i: 173, loss: 3.7341084480285645, loss_memory: 1.8782004190143198e-05, loss_input: 3.7340896129608154\n",
      "it: 0, i: 174, loss: 3.514810085296631, loss_memory: 1.861018972704187e-05, loss_input: 3.514791488647461\n",
      "it: 0, i: 175, loss: 3.6291422843933105, loss_memory: 1.8454255041433498e-05, loss_input: 3.6291239261627197\n",
      "it: 0, i: 176, loss: 3.7673709392547607, loss_memory: 1.8277149138157256e-05, loss_input: 3.76735258102417\n",
      "it: 0, i: 177, loss: 3.524135112762451, loss_memory: 1.8119562810170464e-05, loss_input: 3.5241169929504395\n",
      "it: 0, i: 178, loss: 3.8403677940368652, loss_memory: 1.7982743884203956e-05, loss_input: 3.8403499126434326\n",
      "it: 0, i: 179, loss: 3.806999444961548, loss_memory: 1.7831958757597022e-05, loss_input: 3.8069815635681152\n",
      "it: 0, i: 180, loss: 3.682737112045288, loss_memory: 1.767775756889023e-05, loss_input: 3.6827194690704346\n",
      "it: 0, i: 181, loss: 3.8843860626220703, loss_memory: 1.7526544979773462e-05, loss_input: 3.884368419647217\n",
      "it: 0, i: 182, loss: 3.6264750957489014, loss_memory: 1.7353868315694854e-05, loss_input: 3.626457691192627\n",
      "it: 0, i: 183, loss: 3.784961700439453, loss_memory: 1.719712781778071e-05, loss_input: 3.784944534301758\n",
      "it: 0, i: 184, loss: 3.7458817958831787, loss_memory: 1.7070753528969362e-05, loss_input: 3.7458646297454834\n",
      "it: 0, i: 185, loss: 3.514857053756714, loss_memory: 1.694435195531696e-05, loss_input: 3.5148401260375977\n",
      "it: 0, i: 186, loss: 3.881523370742798, loss_memory: 1.6804780898382887e-05, loss_input: 3.8815066814422607\n",
      "it: 0, i: 187, loss: 3.6372885704040527, loss_memory: 1.668038203206379e-05, loss_input: 3.6372718811035156\n",
      "it: 0, i: 188, loss: 3.846388101577759, loss_memory: 1.6529746062587947e-05, loss_input: 3.846371650695801\n",
      "it: 0, i: 189, loss: 3.6713643074035645, loss_memory: 1.6411810065619648e-05, loss_input: 3.6713478565216064\n",
      "it: 0, i: 190, loss: 3.499401330947876, loss_memory: 1.6271049389615655e-05, loss_input: 3.499385118484497\n",
      "it: 0, i: 191, loss: 3.6739084720611572, loss_memory: 1.614970460650511e-05, loss_input: 3.6738922595977783\n",
      "it: 0, i: 192, loss: 3.7015345096588135, loss_memory: 1.601454641786404e-05, loss_input: 3.7015185356140137\n",
      "it: 0, i: 193, loss: 3.6582565307617188, loss_memory: 1.5897108823992312e-05, loss_input: 3.658240556716919\n",
      "it: 0, i: 194, loss: 3.806157112121582, loss_memory: 1.5765835996717215e-05, loss_input: 3.8061413764953613\n",
      "it: 0, i: 195, loss: 3.565969467163086, loss_memory: 1.5638101103832014e-05, loss_input: 3.5659537315368652\n",
      "it: 0, i: 196, loss: 3.7431235313415527, loss_memory: 1.552884714328684e-05, loss_input: 3.743108034133911\n",
      "it: 0, i: 197, loss: 3.701087713241577, loss_memory: 1.5418905604747124e-05, loss_input: 3.7010722160339355\n",
      "it: 0, i: 198, loss: 3.5812244415283203, loss_memory: 1.5292247553588822e-05, loss_input: 3.581209182739258\n",
      "it: 0, i: 199, loss: 3.6245925426483154, loss_memory: 1.5182101378741208e-05, loss_input: 3.624577283859253\n",
      "it: 0, i: 200, loss: 3.5647311210632324, loss_memory: 1.5050831279950216e-05, loss_input: 3.564716100692749\n",
      "it: 0, i: 201, loss: 3.425333261489868, loss_memory: 1.4938268577679992e-05, loss_input: 3.4253182411193848\n",
      "it: 0, i: 202, loss: 3.588829517364502, loss_memory: 1.4824406207480934e-05, loss_input: 3.5888147354125977\n",
      "it: 0, i: 203, loss: 3.4935197830200195, loss_memory: 1.4735767763340846e-05, loss_input: 3.4935050010681152\n",
      "it: 0, i: 204, loss: 3.567474842071533, loss_memory: 1.4650473531219177e-05, loss_input: 3.567460298538208\n",
      "it: 0, i: 205, loss: 3.7270030975341797, loss_memory: 1.4540670235874131e-05, loss_input: 3.7269885540008545\n",
      "it: 0, i: 206, loss: 3.454655408859253, loss_memory: 1.4410912626772188e-05, loss_input: 3.454641103744507\n",
      "it: 0, i: 207, loss: 3.5638790130615234, loss_memory: 1.4297725101641845e-05, loss_input: 3.5638647079467773\n",
      "it: 0, i: 208, loss: 3.5578277111053467, loss_memory: 1.4217623174772598e-05, loss_input: 3.5578134059906006\n",
      "it: 0, i: 209, loss: 3.696518659591675, loss_memory: 1.412186975358054e-05, loss_input: 3.696504592895508\n",
      "it: 0, i: 210, loss: 3.7275898456573486, loss_memory: 1.4015966371516697e-05, loss_input: 3.7275757789611816\n",
      "it: 0, i: 211, loss: 3.645209550857544, loss_memory: 1.391036857967265e-05, loss_input: 3.645195722579956\n",
      "it: 0, i: 212, loss: 3.5097122192382812, loss_memory: 1.3798891814076342e-05, loss_input: 3.5096983909606934\n",
      "it: 0, i: 213, loss: 3.7172281742095947, loss_memory: 1.372954466205556e-05, loss_input: 3.717214345932007\n",
      "it: 0, i: 214, loss: 3.743100881576538, loss_memory: 1.3636128642247058e-05, loss_input: 3.7430872917175293\n",
      "it: 0, i: 215, loss: 3.718998670578003, loss_memory: 1.3553304597735405e-05, loss_input: 3.718985080718994\n",
      "it: 0, i: 216, loss: 3.63920259475708, loss_memory: 1.3460617992677726e-05, loss_input: 3.6391892433166504\n",
      "it: 0, i: 217, loss: 3.6419029235839844, loss_memory: 1.3370654414757155e-05, loss_input: 3.6418895721435547\n",
      "it: 0, i: 218, loss: 3.705454111099243, loss_memory: 1.3286971807247028e-05, loss_input: 3.7054407596588135\n",
      "it: 0, i: 219, loss: 3.5782692432403564, loss_memory: 1.3196959116612561e-05, loss_input: 3.578256130218506\n",
      "it: 0, i: 220, loss: 3.747857093811035, loss_memory: 1.3083092198939994e-05, loss_input: 3.7478439807891846\n",
      "it: 0, i: 221, loss: 3.680267810821533, loss_memory: 1.2992017218493856e-05, loss_input: 3.6802549362182617\n",
      "it: 0, i: 222, loss: 3.5056724548339844, loss_memory: 1.2919801520183682e-05, loss_input: 3.505659580230713\n",
      "it: 0, i: 223, loss: 3.604091167449951, loss_memory: 1.2828810213250108e-05, loss_input: 3.6040782928466797\n",
      "it: 0, i: 224, loss: 3.465412139892578, loss_memory: 1.2733570656564552e-05, loss_input: 3.4653995037078857\n",
      "it: 0, i: 225, loss: 3.6086013317108154, loss_memory: 1.2694409633695614e-05, loss_input: 3.608588695526123\n",
      "it: 0, i: 226, loss: 3.5711264610290527, loss_memory: 1.2621823771041818e-05, loss_input: 3.5711138248443604\n",
      "it: 0, i: 227, loss: 3.596147298812866, loss_memory: 1.2526819773484021e-05, loss_input: 3.596134662628174\n",
      "it: 0, i: 228, loss: 3.467083692550659, loss_memory: 1.244655140908435e-05, loss_input: 3.467071294784546\n",
      "it: 0, i: 229, loss: 3.7038402557373047, loss_memory: 1.2353126294328831e-05, loss_input: 3.7038278579711914\n",
      "it: 0, i: 230, loss: 3.6605749130249023, loss_memory: 1.2267627425899263e-05, loss_input: 3.660562753677368\n",
      "it: 0, i: 231, loss: 3.4555742740631104, loss_memory: 1.221212005475536e-05, loss_input: 3.455562114715576\n",
      "it: 0, i: 232, loss: 3.621178150177002, loss_memory: 1.2152759154560044e-05, loss_input: 3.6211659908294678\n",
      "it: 0, i: 233, loss: 3.557103395462036, loss_memory: 1.2051343219354749e-05, loss_input: 3.557091236114502\n",
      "it: 0, i: 234, loss: 3.5016860961914062, loss_memory: 1.199348389491206e-05, loss_input: 3.501674175262451\n",
      "it: 0, i: 235, loss: 3.477308988571167, loss_memory: 1.1905038263648748e-05, loss_input: 3.477297067642212\n",
      "it: 0, i: 236, loss: 3.7167179584503174, loss_memory: 1.1832398740807548e-05, loss_input: 3.7167060375213623\n",
      "it: 0, i: 237, loss: 3.4200336933135986, loss_memory: 1.1780981367337517e-05, loss_input: 3.4200220108032227\n",
      "it: 0, i: 238, loss: 3.417557716369629, loss_memory: 1.1716729204636067e-05, loss_input: 3.417546033859253\n",
      "it: 0, i: 239, loss: 3.500373363494873, loss_memory: 1.1639211152214557e-05, loss_input: 3.500361680984497\n",
      "it: 0, i: 240, loss: 3.5763068199157715, loss_memory: 1.1556341632967815e-05, loss_input: 3.5762953758239746\n",
      "it: 0, i: 241, loss: 3.575881242752075, loss_memory: 1.1489355529192835e-05, loss_input: 3.5758697986602783\n",
      "it: 0, i: 242, loss: 3.613072156906128, loss_memory: 1.1421871022321284e-05, loss_input: 3.613060712814331\n",
      "it: 0, i: 243, loss: 3.5610289573669434, loss_memory: 1.1363445082679391e-05, loss_input: 3.5610175132751465\n",
      "it: 0, i: 244, loss: 3.5928452014923096, loss_memory: 1.1321228157612495e-05, loss_input: 3.592833995819092\n",
      "it: 0, i: 245, loss: 3.607053518295288, loss_memory: 1.122387038776651e-05, loss_input: 3.6070423126220703\n",
      "it: 0, i: 246, loss: 3.7419817447662354, loss_memory: 1.1166198419232387e-05, loss_input: 3.7419705390930176\n",
      "it: 0, i: 247, loss: 3.6445183753967285, loss_memory: 1.1108573744422756e-05, loss_input: 3.6445071697235107\n",
      "it: 0, i: 248, loss: 3.5867557525634766, loss_memory: 1.1061177247029264e-05, loss_input: 3.586744785308838\n",
      "it: 0, i: 249, loss: 3.6116902828216553, loss_memory: 1.099378368962789e-05, loss_input: 3.6116793155670166\n",
      "it: 0, i: 250, loss: 3.7842469215393066, loss_memory: 1.0932883014902472e-05, loss_input: 3.784235954284668\n",
      "it: 0, i: 251, loss: 3.607694149017334, loss_memory: 1.0899561857513618e-05, loss_input: 3.6076831817626953\n",
      "it: 0, i: 252, loss: 3.7710392475128174, loss_memory: 1.080083256965736e-05, loss_input: 3.771028518676758\n",
      "it: 0, i: 253, loss: 3.6796658039093018, loss_memory: 1.0757410564110614e-05, loss_input: 3.679655075073242\n",
      "it: 0, i: 254, loss: 3.5960967540740967, loss_memory: 1.0707321052905172e-05, loss_input: 3.596086025238037\n",
      "it: 0, i: 255, loss: 3.796323299407959, loss_memory: 1.0623383786878549e-05, loss_input: 3.7963125705718994\n",
      "it: 0, i: 256, loss: 3.4190425872802734, loss_memory: 1.0557089808571618e-05, loss_input: 3.419032096862793\n",
      "it: 0, i: 257, loss: 3.8090243339538574, loss_memory: 1.0517136615817435e-05, loss_input: 3.809013843536377\n",
      "it: 0, i: 258, loss: 3.5762336254119873, loss_memory: 1.042269741446944e-05, loss_input: 3.576223134994507\n",
      "it: 0, i: 259, loss: 3.7132203578948975, loss_memory: 1.0407904483145103e-05, loss_input: 3.713209867477417\n",
      "it: 0, i: 260, loss: 3.4731245040893555, loss_memory: 1.0340833796362858e-05, loss_input: 3.473114252090454\n",
      "it: 0, i: 261, loss: 3.640575647354126, loss_memory: 1.0287444638379384e-05, loss_input: 3.6405653953552246\n",
      "it: 0, i: 262, loss: 3.613321304321289, loss_memory: 1.0252738320559729e-05, loss_input: 3.6133110523223877\n",
      "it: 0, i: 263, loss: 3.5081748962402344, loss_memory: 1.0207131708739325e-05, loss_input: 3.508164644241333\n",
      "it: 0, i: 264, loss: 3.7165615558624268, loss_memory: 1.0152206414204556e-05, loss_input: 3.7165513038635254\n",
      "it: 0, i: 265, loss: 3.5949244499206543, loss_memory: 1.0082204425998498e-05, loss_input: 3.594914436340332\n",
      "it: 0, i: 266, loss: 3.5676355361938477, loss_memory: 1.0045581802842207e-05, loss_input: 3.5676255226135254\n",
      "it: 0, i: 267, loss: 3.8216445446014404, loss_memory: 9.989464160753414e-06, loss_input: 3.821634531021118\n",
      "it: 0, i: 268, loss: 3.5834789276123047, loss_memory: 9.902132660499774e-06, loss_input: 3.5834689140319824\n",
      "it: 0, i: 269, loss: 3.518986225128174, loss_memory: 9.858746125246398e-06, loss_input: 3.5189764499664307\n",
      "it: 0, i: 270, loss: 3.4816701412200928, loss_memory: 9.834469892666675e-06, loss_input: 3.4816603660583496\n",
      "it: 0, i: 271, loss: 3.8167483806610107, loss_memory: 9.767461961018853e-06, loss_input: 3.8167386054992676\n",
      "it: 0, i: 272, loss: 3.6637837886810303, loss_memory: 9.723371476866305e-06, loss_input: 3.663774013519287\n",
      "it: 0, i: 273, loss: 3.4924023151397705, loss_memory: 9.68358654063195e-06, loss_input: 3.4923925399780273\n",
      "it: 0, i: 274, loss: 3.6225638389587402, loss_memory: 9.6208223112626e-06, loss_input: 3.622554302215576\n",
      "it: 0, i: 275, loss: 3.6191015243530273, loss_memory: 9.594728908268735e-06, loss_input: 3.6190919876098633\n",
      "it: 0, i: 276, loss: 3.6415085792541504, loss_memory: 9.52717618929455e-06, loss_input: 3.6414990425109863\n",
      "it: 0, i: 277, loss: 3.566859245300293, loss_memory: 9.486476301390212e-06, loss_input: 3.566849708557129\n",
      "it: 0, i: 278, loss: 3.6307032108306885, loss_memory: 9.456451152800582e-06, loss_input: 3.6306936740875244\n",
      "it: 0, i: 279, loss: 3.518631935119629, loss_memory: 9.406379831489176e-06, loss_input: 3.518622636795044\n",
      "it: 0, i: 280, loss: 3.6276397705078125, loss_memory: 9.347329068987165e-06, loss_input: 3.6276304721832275\n",
      "it: 0, i: 281, loss: 3.5682241916656494, loss_memory: 9.312385373050347e-06, loss_input: 3.5682148933410645\n",
      "it: 0, i: 282, loss: 3.8133833408355713, loss_memory: 9.284917723562103e-06, loss_input: 3.8133740425109863\n",
      "it: 0, i: 283, loss: 3.480099678039551, loss_memory: 9.244266038876958e-06, loss_input: 3.480090379714966\n",
      "it: 0, i: 284, loss: 3.681692123413086, loss_memory: 9.180906999972649e-06, loss_input: 3.681682825088501\n",
      "it: 0, i: 285, loss: 3.466489315032959, loss_memory: 9.155421139439568e-06, loss_input: 3.466480255126953\n",
      "it: 0, i: 286, loss: 3.6497995853424072, loss_memory: 9.109826351050287e-06, loss_input: 3.6497905254364014\n",
      "it: 0, i: 287, loss: 3.470033645629883, loss_memory: 9.06489003682509e-06, loss_input: 3.470024585723877\n",
      "it: 0, i: 288, loss: 3.562980890274048, loss_memory: 8.989569323603064e-06, loss_input: 3.562971830368042\n",
      "it: 0, i: 289, loss: 3.560833692550659, loss_memory: 8.965987944975495e-06, loss_input: 3.5608246326446533\n",
      "it: 0, i: 290, loss: 3.424807548522949, loss_memory: 8.938160135585349e-06, loss_input: 3.4247987270355225\n",
      "it: 0, i: 291, loss: 3.544231653213501, loss_memory: 8.913371857488528e-06, loss_input: 3.544222831726074\n",
      "it: 0, i: 292, loss: 3.46140456199646, loss_memory: 8.86224825080717e-06, loss_input: 3.461395740509033\n",
      "it: 0, i: 293, loss: 3.6612441539764404, loss_memory: 8.815060027700383e-06, loss_input: 3.6612353324890137\n",
      "it: 0, i: 294, loss: 3.4736552238464355, loss_memory: 8.769677151576616e-06, loss_input: 3.473646402359009\n",
      "it: 0, i: 295, loss: 3.6110446453094482, loss_memory: 8.73963836056646e-06, loss_input: 3.6110358238220215\n",
      "it: 0, i: 296, loss: 3.673783540725708, loss_memory: 8.685155989951454e-06, loss_input: 3.6737749576568604\n",
      "it: 0, i: 297, loss: 3.4181764125823975, loss_memory: 8.650502422824502e-06, loss_input: 3.41816782951355\n",
      "it: 0, i: 298, loss: 3.436537504196167, loss_memory: 8.61430999066215e-06, loss_input: 3.4365289211273193\n",
      "it: 0, i: 299, loss: 3.360372304916382, loss_memory: 8.57574832480168e-06, loss_input: 3.360363721847534\n",
      "it: 0, i: 300, loss: 3.6439945697784424, loss_memory: 8.565056305087637e-06, loss_input: 3.6439859867095947\n",
      "it: 0, i: 301, loss: 3.436593532562256, loss_memory: 8.492873348586727e-06, loss_input: 3.436584949493408\n",
      "it: 0, i: 302, loss: 3.632822275161743, loss_memory: 8.482819794153329e-06, loss_input: 3.6328136920928955\n",
      "it: 0, i: 303, loss: 3.6514999866485596, loss_memory: 8.450237146462314e-06, loss_input: 3.651491641998291\n",
      "it: 0, i: 304, loss: 3.4439172744750977, loss_memory: 8.3984696175321e-06, loss_input: 3.443908929824829\n",
      "it: 0, i: 305, loss: 3.5575499534606934, loss_memory: 8.37151128507685e-06, loss_input: 3.557541608810425\n",
      "it: 0, i: 306, loss: 3.47221040725708, loss_memory: 8.331291610375047e-06, loss_input: 3.4722020626068115\n",
      "it: 0, i: 307, loss: 3.4940543174743652, loss_memory: 8.285512194561306e-06, loss_input: 3.4940459728240967\n",
      "it: 0, i: 308, loss: 3.526411533355713, loss_memory: 8.265857104561292e-06, loss_input: 3.5264031887054443\n",
      "it: 0, i: 309, loss: 3.527827501296997, loss_memory: 8.213759429054335e-06, loss_input: 3.5278193950653076\n",
      "it: 0, i: 310, loss: 3.4016013145446777, loss_memory: 8.193414032575674e-06, loss_input: 3.4015932083129883\n",
      "it: 0, i: 311, loss: 3.5123438835144043, loss_memory: 8.151214387908112e-06, loss_input: 3.512335777282715\n",
      "it: 0, i: 312, loss: 3.6498806476593018, loss_memory: 8.109453119686805e-06, loss_input: 3.6498725414276123\n",
      "it: 0, i: 313, loss: 3.4483749866485596, loss_memory: 8.08635559224058e-06, loss_input: 3.44836688041687\n",
      "it: 0, i: 314, loss: 3.4452362060546875, loss_memory: 8.068895112955943e-06, loss_input: 3.445228099822998\n",
      "it: 0, i: 315, loss: 3.5802202224731445, loss_memory: 8.013359547476284e-06, loss_input: 3.580212116241455\n",
      "it: 0, i: 316, loss: 3.5424611568450928, loss_memory: 7.991213351488113e-06, loss_input: 3.5424530506134033\n",
      "it: 0, i: 317, loss: 3.5717647075653076, loss_memory: 7.954742613947019e-06, loss_input: 3.5717568397521973\n",
      "it: 0, i: 318, loss: 3.649789571762085, loss_memory: 7.92150149209192e-06, loss_input: 3.6497817039489746\n",
      "it: 0, i: 319, loss: 3.541806697845459, loss_memory: 7.90568083175458e-06, loss_input: 3.5417988300323486\n",
      "it: 0, i: 320, loss: 3.605773448944092, loss_memory: 7.868229658924975e-06, loss_input: 3.6057655811309814\n",
      "it: 0, i: 321, loss: 3.458373785018921, loss_memory: 7.857355740270577e-06, loss_input: 3.4583659172058105\n",
      "it: 0, i: 322, loss: 3.569225311279297, loss_memory: 7.783550245221704e-06, loss_input: 3.5692174434661865\n",
      "it: 0, i: 323, loss: 3.711237668991089, loss_memory: 7.770925549266394e-06, loss_input: 3.7112298011779785\n",
      "it: 0, i: 324, loss: 3.780611276626587, loss_memory: 7.745173206785694e-06, loss_input: 3.7806036472320557\n",
      "it: 0, i: 325, loss: 3.588778257369995, loss_memory: 7.706191354373004e-06, loss_input: 3.588770627975464\n",
      "it: 0, i: 326, loss: 3.5687012672424316, loss_memory: 7.678921974729747e-06, loss_input: 3.5686936378479004\n",
      "it: 0, i: 327, loss: 3.6276848316192627, loss_memory: 7.634795110789128e-06, loss_input: 3.6276772022247314\n",
      "it: 0, i: 328, loss: 3.609130382537842, loss_memory: 7.630749678355642e-06, loss_input: 3.6091227531433105\n",
      "it: 0, i: 329, loss: 3.671112298965454, loss_memory: 7.612150511704385e-06, loss_input: 3.671104669570923\n",
      "it: 0, i: 330, loss: 3.692725658416748, loss_memory: 7.55929431761615e-06, loss_input: 3.692718029022217\n",
      "it: 0, i: 331, loss: 3.5639753341674805, loss_memory: 7.541687409684528e-06, loss_input: 3.563967704772949\n",
      "it: 0, i: 332, loss: 3.53171443939209, loss_memory: 7.513584478147095e-06, loss_input: 3.5317068099975586\n",
      "it: 0, i: 333, loss: 3.5991177558898926, loss_memory: 7.486600225092843e-06, loss_input: 3.5991103649139404\n",
      "it: 0, i: 334, loss: 3.585446834564209, loss_memory: 7.459439984813798e-06, loss_input: 3.585439443588257\n",
      "it: 0, i: 335, loss: 3.7500855922698975, loss_memory: 7.411741080431966e-06, loss_input: 3.7500782012939453\n",
      "it: 0, i: 336, loss: 3.5874671936035156, loss_memory: 7.395797638309887e-06, loss_input: 3.5874598026275635\n",
      "it: 0, i: 337, loss: 3.563190221786499, loss_memory: 7.375222594419029e-06, loss_input: 3.563182830810547\n",
      "it: 0, i: 338, loss: 3.5527608394622803, loss_memory: 7.330248990911059e-06, loss_input: 3.552753448486328\n",
      "it: 0, i: 339, loss: 3.663748025894165, loss_memory: 7.2977281888597645e-06, loss_input: 3.663740634918213\n",
      "it: 0, i: 340, loss: 3.4809365272521973, loss_memory: 7.292387181223603e-06, loss_input: 3.480929136276245\n",
      "it: 0, i: 341, loss: 3.4750707149505615, loss_memory: 7.259853191499133e-06, loss_input: 3.4750635623931885\n",
      "it: 0, i: 342, loss: 3.4831433296203613, loss_memory: 7.240874765557237e-06, loss_input: 3.4831361770629883\n",
      "it: 0, i: 343, loss: 3.4410505294799805, loss_memory: 7.215749064926058e-06, loss_input: 3.4410433769226074\n",
      "it: 0, i: 344, loss: 3.5791666507720947, loss_memory: 7.186936272773892e-06, loss_input: 3.5791594982147217\n",
      "it: 0, i: 345, loss: 3.7088913917541504, loss_memory: 7.15520172889228e-06, loss_input: 3.7088842391967773\n",
      "it: 0, i: 346, loss: 3.5570480823516846, loss_memory: 7.124673174985219e-06, loss_input: 3.5570409297943115\n",
      "it: 0, i: 347, loss: 3.6175904273986816, loss_memory: 7.10491849531536e-06, loss_input: 3.6175832748413086\n",
      "it: 0, i: 348, loss: 3.7162017822265625, loss_memory: 7.079255283315433e-06, loss_input: 3.7161946296691895\n",
      "it: 0, i: 349, loss: 3.516127586364746, loss_memory: 7.090076906024478e-06, loss_input: 3.516120433807373\n",
      "it: 0, i: 350, loss: 3.585479497909546, loss_memory: 7.033394012978533e-06, loss_input: 3.585472345352173\n",
      "it: 0, i: 351, loss: 3.6445107460021973, loss_memory: 7.014500624791253e-06, loss_input: 3.6445038318634033\n",
      "it: 0, i: 352, loss: 3.402107000350952, loss_memory: 6.976012173254276e-06, loss_input: 3.402100086212158\n",
      "it: 0, i: 353, loss: 3.555541515350342, loss_memory: 6.972401479288237e-06, loss_input: 3.555534601211548\n",
      "it: 0, i: 354, loss: 3.582801580429077, loss_memory: 6.958473022677936e-06, loss_input: 3.582794666290283\n",
      "it: 0, i: 355, loss: 3.4431517124176025, loss_memory: 6.92203775543021e-06, loss_input: 3.4431447982788086\n",
      "it: 0, i: 356, loss: 3.4294471740722656, loss_memory: 6.902809673192678e-06, loss_input: 3.4294402599334717\n",
      "it: 0, i: 357, loss: 3.5505623817443848, loss_memory: 6.845290045021102e-06, loss_input: 3.550555467605591\n",
      "it: 0, i: 358, loss: 3.573435068130493, loss_memory: 6.821702754677972e-06, loss_input: 3.573428153991699\n",
      "it: 0, i: 359, loss: 3.5580081939697266, loss_memory: 6.820290764153469e-06, loss_input: 3.5580012798309326\n",
      "it: 0, i: 360, loss: 3.5455524921417236, loss_memory: 6.802661118854303e-06, loss_input: 3.5455455780029297\n",
      "it: 0, i: 361, loss: 3.413656711578369, loss_memory: 6.781866431992967e-06, loss_input: 3.4136500358581543\n",
      "it: 0, i: 362, loss: 3.5473055839538574, loss_memory: 6.7612600105348974e-06, loss_input: 3.5472989082336426\n",
      "it: 0, i: 363, loss: 3.465651035308838, loss_memory: 6.7180462792748585e-06, loss_input: 3.465644359588623\n",
      "it: 0, i: 364, loss: 3.6467134952545166, loss_memory: 6.7324012889002915e-06, loss_input: 3.6467068195343018\n",
      "it: 0, i: 365, loss: 3.48347544670105, loss_memory: 6.690536793030333e-06, loss_input: 3.483468770980835\n",
      "it: 0, i: 366, loss: 3.484121799468994, loss_memory: 6.673245479760226e-06, loss_input: 3.4841151237487793\n",
      "it: 0, i: 367, loss: 3.4855475425720215, loss_memory: 6.641328127443558e-06, loss_input: 3.4855408668518066\n",
      "it: 0, i: 368, loss: 3.4030017852783203, loss_memory: 6.641050276812166e-06, loss_input: 3.4029951095581055\n",
      "it: 0, i: 369, loss: 3.5085630416870117, loss_memory: 6.5931444623856805e-06, loss_input: 3.508556365966797\n",
      "it: 0, i: 370, loss: 3.447139024734497, loss_memory: 6.602182565984549e-06, loss_input: 3.4471323490142822\n",
      "it: 0, i: 371, loss: 3.3525404930114746, loss_memory: 6.58714725432219e-06, loss_input: 3.3525338172912598\n",
      "it: 0, i: 372, loss: 3.455291271209717, loss_memory: 6.528966878249776e-06, loss_input: 3.455284833908081\n",
      "it: 0, i: 373, loss: 3.4498159885406494, loss_memory: 6.513460903079249e-06, loss_input: 3.4498095512390137\n",
      "it: 0, i: 374, loss: 3.3259360790252686, loss_memory: 6.529266102006659e-06, loss_input: 3.325929641723633\n",
      "it: 0, i: 375, loss: 3.403045177459717, loss_memory: 6.490029591077473e-06, loss_input: 3.403038740158081\n",
      "it: 0, i: 376, loss: 3.5520102977752686, loss_memory: 6.4253858909069095e-06, loss_input: 3.552003860473633\n",
      "it: 0, i: 377, loss: 3.549769401550293, loss_memory: 6.4147156990657095e-06, loss_input: 3.5497629642486572\n",
      "it: 0, i: 378, loss: 3.6907596588134766, loss_memory: 6.404319719877094e-06, loss_input: 3.690753221511841\n",
      "it: 0, i: 379, loss: 3.6771364212036133, loss_memory: 6.3791126194701064e-06, loss_input: 3.6771299839019775\n",
      "it: 0, i: 380, loss: 3.6762704849243164, loss_memory: 6.383399522746913e-06, loss_input: 3.6762640476226807\n",
      "it: 0, i: 381, loss: 3.5862464904785156, loss_memory: 6.35464493825566e-06, loss_input: 3.58624005317688\n",
      "it: 0, i: 382, loss: 3.677035093307495, loss_memory: 6.353144726745086e-06, loss_input: 3.6770286560058594\n",
      "it: 0, i: 383, loss: 3.738072156906128, loss_memory: 6.343514542095363e-06, loss_input: 3.738065719604492\n",
      "it: 0, i: 384, loss: 3.4571430683135986, loss_memory: 6.2854437601345126e-06, loss_input: 3.457136869430542\n",
      "it: 0, i: 385, loss: 3.7078983783721924, loss_memory: 6.284513347054599e-06, loss_input: 3.7078921794891357\n",
      "it: 0, i: 386, loss: 3.371037244796753, loss_memory: 6.2599424381915014e-06, loss_input: 3.3710310459136963\n",
      "it: 0, i: 387, loss: 3.5053977966308594, loss_memory: 6.226998721103882e-06, loss_input: 3.5053915977478027\n",
      "it: 0, i: 388, loss: 3.4917244911193848, loss_memory: 6.245766599022318e-06, loss_input: 3.491718292236328\n",
      "it: 0, i: 389, loss: 3.4927268028259277, loss_memory: 6.224498065421358e-06, loss_input: 3.492720603942871\n",
      "it: 0, i: 390, loss: 3.3969109058380127, loss_memory: 6.183358891576063e-06, loss_input: 3.396904706954956\n",
      "it: 0, i: 391, loss: 3.4921987056732178, loss_memory: 6.147700787551003e-06, loss_input: 3.492192506790161\n",
      "it: 0, i: 392, loss: 3.368068218231201, loss_memory: 6.1819382608518936e-06, loss_input: 3.3680620193481445\n",
      "it: 0, i: 393, loss: 3.722329616546631, loss_memory: 6.154912625788711e-06, loss_input: 3.722323417663574\n",
      "it: 0, i: 394, loss: 3.4607136249542236, loss_memory: 6.110750291554723e-06, loss_input: 3.460707426071167\n",
      "it: 0, i: 395, loss: 3.626293659210205, loss_memory: 6.0900483731529675e-06, loss_input: 3.6262874603271484\n",
      "it: 0, i: 396, loss: 3.7226064205169678, loss_memory: 6.0687520999636035e-06, loss_input: 3.7226004600524902\n",
      "it: 0, i: 397, loss: 3.6392621994018555, loss_memory: 6.069098162697628e-06, loss_input: 3.639256238937378\n",
      "it: 0, i: 398, loss: 3.5776143074035645, loss_memory: 6.030734766682144e-06, loss_input: 3.577608346939087\n",
      "it: 0, i: 399, loss: 3.470088481903076, loss_memory: 6.0384581956896e-06, loss_input: 3.4700825214385986\n",
      "it: 0, i: 400, loss: 3.4942946434020996, loss_memory: 6.01596366323065e-06, loss_input: 3.494288682937622\n",
      "it: 0, i: 401, loss: 3.467863082885742, loss_memory: 6.007554020470707e-06, loss_input: 3.4678571224212646\n",
      "it: 0, i: 402, loss: 3.4697747230529785, loss_memory: 5.984718882245943e-06, loss_input: 3.469768762588501\n",
      "it: 0, i: 403, loss: 3.6482419967651367, loss_memory: 5.939604307059199e-06, loss_input: 3.648236036300659\n",
      "it: 0, i: 404, loss: 3.6325459480285645, loss_memory: 5.976510692562442e-06, loss_input: 3.632539987564087\n",
      "it: 0, i: 405, loss: 3.470323324203491, loss_memory: 5.927262918703491e-06, loss_input: 3.4703173637390137\n",
      "it: 0, i: 406, loss: 3.6059579849243164, loss_memory: 5.895391041121911e-06, loss_input: 3.605952024459839\n",
      "it: 0, i: 407, loss: 3.6167991161346436, loss_memory: 5.87994918532786e-06, loss_input: 3.616793155670166\n",
      "it: 0, i: 408, loss: 3.5607261657714844, loss_memory: 5.900642463529948e-06, loss_input: 3.560720205307007\n",
      "it: 0, i: 409, loss: 3.6917407512664795, loss_memory: 5.8359428294352256e-06, loss_input: 3.691735029220581\n",
      "it: 0, i: 410, loss: 3.6003737449645996, loss_memory: 5.8335917856311426e-06, loss_input: 3.600368022918701\n",
      "it: 0, i: 411, loss: 3.537328004837036, loss_memory: 5.839014647790464e-06, loss_input: 3.5373222827911377\n",
      "it: 0, i: 412, loss: 3.456223964691162, loss_memory: 5.817361397930654e-06, loss_input: 3.4562182426452637\n",
      "it: 0, i: 413, loss: 3.4567599296569824, loss_memory: 5.789507667941507e-06, loss_input: 3.456754207611084\n",
      "it: 0, i: 414, loss: 3.407163143157959, loss_memory: 5.7607312555774115e-06, loss_input: 3.4071574211120605\n",
      "it: 0, i: 415, loss: 3.705833911895752, loss_memory: 5.763415174442343e-06, loss_input: 3.7058281898498535\n",
      "it: 0, i: 416, loss: 3.524658679962158, loss_memory: 5.766747563029639e-06, loss_input: 3.5246529579162598\n",
      "it: 0, i: 417, loss: 3.620948076248169, loss_memory: 5.732595582230715e-06, loss_input: 3.6209423542022705\n",
      "it: 0, i: 418, loss: 3.6984243392944336, loss_memory: 5.730167686124332e-06, loss_input: 3.698418617248535\n",
      "it: 0, i: 419, loss: 3.580374002456665, loss_memory: 5.718068678106647e-06, loss_input: 3.5803682804107666\n",
      "it: 0, i: 420, loss: 3.7647156715393066, loss_memory: 5.689497356797801e-06, loss_input: 3.764709949493408\n",
      "it: 0, i: 421, loss: 3.6597962379455566, loss_memory: 5.6752178352326155e-06, loss_input: 3.659790515899658\n",
      "it: 0, i: 422, loss: 3.8105974197387695, loss_memory: 5.686873919330537e-06, loss_input: 3.810591697692871\n",
      "it: 0, i: 423, loss: 3.7308244705200195, loss_memory: 5.646762019750895e-06, loss_input: 3.730818748474121\n",
      "it: 0, i: 424, loss: 3.6973674297332764, loss_memory: 5.646031695505371e-06, loss_input: 3.697361707687378\n",
      "it: 0, i: 425, loss: 3.62565541267395, loss_memory: 5.612251698039472e-06, loss_input: 3.6256496906280518\n",
      "it: 0, i: 426, loss: 3.6302828788757324, loss_memory: 5.594735739578027e-06, loss_input: 3.630277395248413\n",
      "it: 0, i: 427, loss: 3.7444732189178467, loss_memory: 5.573727321461774e-06, loss_input: 3.7444677352905273\n",
      "it: 0, i: 428, loss: 3.7269840240478516, loss_memory: 5.578503078140784e-06, loss_input: 3.7269785404205322\n",
      "it: 0, i: 429, loss: 3.5819146633148193, loss_memory: 5.557809345191345e-06, loss_input: 3.5819091796875\n",
      "it: 0, i: 430, loss: 3.5609607696533203, loss_memory: 5.5199516282300465e-06, loss_input: 3.560955286026001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m loss_input \u001B[38;5;241m=\u001B[39m cross_entropy(pred_input, current_x\u001B[38;5;241m.\u001B[39margmax(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)) \u001B[38;5;66;03m#for the input we use cross entropy loss\u001B[39;00m\n\u001B[1;32m     22\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_memory \u001B[38;5;241m+\u001B[39m loss_input\n\u001B[0;32m---> 23\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     24\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mit: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mit\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, i: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss_memory: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss_memory\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, loss_input: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mloss_input\u001B[38;5;241m.\u001B[39mitem()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/snn-exploration/snn_venv/lib/python3.10/site-packages/torch/_tensor.py:492\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    484\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    485\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    490\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    491\u001B[0m     )\n\u001B[0;32m--> 492\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/snn-exploration/snn_venv/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    246\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 251\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    252\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    256\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    257\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    258\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    259\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "net = Net(\n",
    "    batch_size=batch_size,\n",
    "    input_size=256,\n",
    "    memory_size=memory_size,\n",
    "    device=device\n",
    ")\n",
    "import torch\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "cross_entropy = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for it in range(1000):\n",
    "    x, _ = generator.get_batch()\n",
    "    for i in range(history_size): #min(it,history_size)): #maybe curriculum learning by increasing this number slowly over time because of the issue that m0 is not trained to be resilient against noise and can only be indirectly trained by the memory\n",
    "        optimizer.zero_grad()\n",
    "        current_x = x[:, i, :] # (batch_size, 256) #target value for pred_input\n",
    "        old_mem = net.memory.clone().detach()\n",
    "        mem = net(current_x) #this is the current memory that has been produced by the network\n",
    "        pred_memory, pred_input = net.reconstruct(noise = 0.01) #pred_memory is in -1, 1 and pred_input is logits\n",
    "        loss_memory = torch.mean((old_mem - pred_memory) ** 2) #for the memory we use mse loss\n",
    "        loss_input = cross_entropy(pred_input, current_x.argmax(dim=1)) #for the input we use cross entropy loss\n",
    "        loss = loss_memory + loss_input\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f'it: {it}, i: {i}, loss: {loss.item()}, loss_memory: {loss_memory.item()}, loss_input: {loss_input.item()}')\n",
    "        \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-07T23:09:00.348120190Z",
     "start_time": "2024-04-07T23:08:56.370843849Z"
    }
   },
   "id": "6d081841ad22c63c",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Problems we are running into right now:\n",
    "#-sometimes, the input reconstruction is not learned in favor of learning to reconstruct the memory. maybe we need to add a minimal change loss to the memory.\n",
    "#-the memory does not seem to be able to recover from noise. purely theoretically, the memory is only indirectly trained to be resilient against noise:\n",
    "#    -imagine the memory has a complex nonzero state\n",
    "#idea for a new trial for this kind of network:\n",
    "#input size is a number x\n",
    "#memory size is a number y=x*n\n",
    "#in theory, the memory should be able to store n inputs. we can test that by showing the network n inputs and then see if it can reconstruct them.\n",
    "#this should give us a good idea of how well the memory is able to store information.\n",
    "#it is also a much simpler task than natural language processing and should be easier to debug.\n",
    "\n",
    "#another problem with the memory is that if it is left unbounded, it will grow indefinitely. this is why we added tanh as the activation function for the memory.\n",
    "\n",
    "#another idea is to add a timing thing to the memory, so that the memory can evolve with timing instead of with reading the input, which should be more easy to reverse. ah. this is a good idea. we can add a timing network that takes the old memory state and\n",
    "#advances the time for the memory. then another network integrates the input to the memory, with a gating mechanism that is controlled by the integration network. \n",
    "#then this new memory state needs to reconstruct the old memory state.\n",
    "#this way, it is observable whether the memory also learns to perform sequence prediction implicitly.\n",
    "#it could also maybe nudge the network more towards doing sequence prediciton, as the temporal evolution of the memory is easier to reconstruct than the input, shifting the memory more towards finding patterns in the input.\n",
    "\n",
    "#to summarize, these are the new ideas from all the problems we have encountered:\n",
    "#-add a minimal change loss to the memory\n",
    "#-add a timing network that advances the memory state:\n",
    "#    -the timing network should take the old memory state and advance the time\n",
    "#    -the integration network should integrate the input to the memory, with a gating mechanism that is controlled by the integration network\n",
    "#    -the new memory state should reconstruct the old memory state\n",
    "#-add a loss that nudges the network more towards doing sequence prediction, as the temporal evolution of the memory is easier to reconstruct than the input, shifting the memory more towards finding patterns in the input\n",
    "\n",
    "#Question: is the memory currently trying to find patterns in the input? like, it needs to compress the previous memory state and the current input into a vector of the same size as the memory state. does it implictly learn to find patterns in the input this way? if it does find patterns in the input sequence, it doesn't need to integrate new information but rather tell the decoding network where it is, so that the decoding network can deconstruct the memory state into the previous memory state and the current input. \n",
    "\n",
    "#how do we make this requirement more explicit? the memory state should be a combination of \"where we are\" for implict knowledge in the decoder, as well as explicit knowledge, like variables. \n",
    "#the implict knowledge might be represented by a multi-state automaton, and the explicit knowledge might be represented by a set of variables whose types and number are a function of the current state of the multi-state automaton.\n",
    "#\"where we are\" is both influenced by the time step and by the input. the variables are timeless and might just change in interpretation based on the \"where we are\" part.\n",
    "#since the number of variables is not fixed, but the memory size is fixed, we will inevitably run into the problem that the memory size is a problem. my hope is that up to a certain degree, this will allow for pattern learning, as the network will have to compress its data into a fixed size."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83de88ea07b2fbbf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
